{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b8761b",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1921ade8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "PyTorch is an open-source machine learning library based on the Torch library, developed by Facebook’s AI Research lab. It is widely used in deep learning, natural language processing, and computer vision applications. PyTorch provides a dynamic computational graph, which allows for easy and efficient modeling of complex neural networks.\n",
    "\n",
    "Here’s a general overview of how to use PyTorch for deep learning:\n",
    "\n",
    "Define the model: PyTorch provides a wide range of pre-built neural network architectures, such as fully connected networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). The model can be defined using PyTorch’s torch.nn module.\n",
    "\n",
    "Define the loss function: The loss function is used to evaluate the model’s performance and update its parameters. PyTorch provides a wide range of loss functions, such as cross-entropy loss and mean squared error loss.\n",
    "\n",
    "Define the optimizer: The optimizer is used to update the model’s parameters based on the gradients of the loss function. PyTorch provides a wide range of optimizers, such as Adam and SGD.\n",
    "\n",
    "Train the model: The model is trained on a dataset using the defined loss function and optimizer. PyTorch provides a convenient data loading and processing library, torch.utils.data, that can be used to load and prepare the data.\n",
    "\n",
    "Evaluate the model: After training, the model’s performance can be evaluated on a test dataset.\n",
    "\n",
    "Deploy the model: The trained model can be deployed in a wide range of applications, such as computer vision and natural language processing.\n",
    "\n",
    " neural net. \n",
    "\n",
    "Tensors: \n",
    "\n",
    "\n",
    "It turns out neural network computations are just a bunch of linear algebra operations on tensors, which are a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor. The fundamental data structure for neural networks are tensors and PyTorch is built around tensors. It’s time to explore how we can use PyTorch to build a simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6afcf137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\comp\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\comp\\anaconda3\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\comp\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\comp\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\comp\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\comp\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\comp\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\comp\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\comp\\anaconda3\\lib\\site-packages (from torchvision) (1.24.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\comp\\anaconda3\\lib\\site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\comp\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\comp\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\comp\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\comp\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\comp\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\comp\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\comp\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\comp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\comp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\comp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\comp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\comp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\comp\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de8e4e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "008413c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "#  1. Basic autograd example 1 \n",
    "# Create tensors.\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a07f032c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  Parameter containing:\n",
      "tensor([[ 0.5100, -0.3348, -0.0840],\n",
      "        [ 0.3025, -0.2229,  0.4414]], requires_grad=True)\n",
      "b:  Parameter containing:\n",
      "tensor([-0.2538, -0.4422], requires_grad=True)\n",
      "loss:  2.192253351211548\n",
      "dL/dw:  tensor([[ 0.2596, -1.3067,  0.1537],\n",
      "        [ 0.1219, -0.6320,  0.2301]])\n",
      "dL/db:  tensor([-0.7705, -0.8655])\n",
      "loss after 1 step optimization:  2.156494617462158\n"
     ]
    }
   ],
   "source": [
    "# 2. Basic autograd example 2 \n",
    "\n",
    "# Create tensors of shape (10, 3) and (10, 2).\n",
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "print ('w: ', linear.weight)\n",
    "print ('b: ', linear.bias)\n",
    "\n",
    "# Build loss function and optimizer.\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# Forward pass.\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('loss: ', loss.item())\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "\n",
    "# 1-step gradient descent.\n",
    "optimizer.step()\n",
    "\n",
    "# You can also perform gradient descent at the low level.\n",
    "# linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "# linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bd4b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   3. Loading data from numpy  \n",
    "\n",
    "# Create a numpy array.\n",
    "x = np.array([[1,2],[3,4]])\n",
    "\n",
    "# Convert the numpy array to a torch tensor.\n",
    "y = torch.from_numpy(x)\n",
    "\n",
    "# Convert the torch tensor to a numpy array.\n",
    "z = y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f02315db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Mini-batch images and labels.\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Download and construct CIFAR-10 dataset.\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)\n",
    "\n",
    "# Fetch one data pair (read data from disk).\n",
    "image, label = train_dataset[0]\n",
    "print(image.size())\n",
    "print(label)\n",
    "\n",
    "# Data loader (this provides queues and threads in a very simple way).\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = next(data_iter)\n",
    "\n",
    "# Actual usage of the data loader is as below.\n",
    "for images, labels in train_loader:\n",
    "    # Training code should be written here.\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7dae2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32]) ('baby-boy.jpg',)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        # Initialize file paths or a list of file names.\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of files in the dataset.\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Read one data from file.\n",
    "        image_path = self.image_paths[index]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        # Apply transformations if any.\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Use the filename or index as a placeholder for label.\n",
    "        label = os.path.basename(image_path)  # Example label using filename.\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define the image path.\n",
    "image_path = [r'C:\\Users\\COMP\\Desktop\\face_rec\\baby-boy.jpg']\n",
    "\n",
    "# Define transformations (e.g., convert to tensor, normalize, etc.).\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # Resize image to 32x32\n",
    "    transforms.ToTensor(),  # Convert image to Tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image\n",
    "])\n",
    "\n",
    "# Initialize the dataset.\n",
    "custom_dataset = CustomDataset(image_paths=image_path, transform=transform)\n",
    "\n",
    "# Use DataLoader to handle batching (though with one image, batching is trivial).\n",
    "train_loader = DataLoader(dataset=custom_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Iterate through the dataset.\n",
    "for images, labels in train_loader:\n",
    "    # Print image tensor and label for verification.\n",
    "    print(images.size(), labels)  # Image size and label (filename).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d2e3975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREtrain model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67309e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# ================================================================== #\n",
    "#                         6. Pretrained Model                        #\n",
    "# ================================================================== #\n",
    "\n",
    "# Download and load the pretrained ResNet-18.\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# If you want to finetune only the top layer of the model, set as below.\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the top layer for finetuning.\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)  # 100 is an example.\n",
    "\n",
    "# Forward pass with a batch of random images.\n",
    "images = torch.randn(64, 3, 224, 224)\n",
    "outputs = resnet(images)\n",
    "print(outputs.size())  # Expected output: (64, 100)\n",
    "\n",
    "# ================================================================== #\n",
    "#                      7. Save and Load the Model                    #\n",
    "# ================================================================== #\n",
    "\n",
    "# Save and load the entire model.\n",
    "torch.save(resnet, 'model.ckpt')\n",
    "loaded_model = torch.load('model.ckpt')\n",
    "\n",
    "# Save and load only the model parameters (recommended).\n",
    "torch.save(resnet.state_dict(), 'params.ckpt')\n",
    "resnet.load_state_dict(torch.load('params.ckpt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66290b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
